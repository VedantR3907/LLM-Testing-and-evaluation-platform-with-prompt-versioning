# Project Title

This is a Streamlit application that integrates various AI models for text generation and evaluation. It provides an interface for users to interact with the models, view their responses, and compare their performance metrics.

## Docs

https://cliff-mechanic-78e.notion.site/LLM-EVALUATION-AND-TESTING-dd5cc930ad424eeb9784a8c3da25c0f7

## Features

- User authentication with Supabase
- Sidebar navigation
- Model selection and interaction
- Display and management of saved prompts
- Visualization of model metrics
- User voting and evaluation

## Installation

1. Clone the repository:
```
git clone https://github.com/yourusername/yourrepository.git
```
3. Navigate to the project directory:

4. Install the required Python packages:
```
pip install -r requirements.txt
```

## Usage


After running the Streamlit application, you can run the command in the src folder as
```
streamlit run  1_üôé‚Äç‚ôÇÔ∏è_UserLogin.py
```

### User Authentication

The application supports user authentication. You can sign up with a new account or log in with an existing one if you want.

### Model Interaction

You can select a model from the sidebar and interact with it. Enter a prompt and the model will generate a response.

### Saved Prompts

You can view and manage saved prompts. The application supports searching for prompts, and you can copy or delete prompts.

### Model Metrics

The application visualizes various metrics for the models, including token counts, cost, and time metrics. You can select a metric from the dropdown menu to view a bar chart of the metric for each model.

### User Voting and Evaluation

Users can vote for models and provide evaluations. The application displays user votes and evaluation results.

## Contributing

Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.

## License

[MIT](https://choosealicense.com/licenses/mit/)
